{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49643686",
   "metadata": {},
   "source": [
    "Reference: https://github.com/ImadSaddik/Train_Your_Language_Model_Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b546d",
   "metadata": {},
   "source": [
    "### Installing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01dcebdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from tokenizers) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\.conda\\envs\\iitr_scu\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.4/2.4 MB 67.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 11.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.21.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "# !pip install tabulate\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  flags                                       instruction category  \\\n",
      "0     B  question about cancelling order {{Order Number}}    ORDER   \n",
      "\n",
      "         intent                                           response  \n",
      "0  cancel_order  I've understood you have a question regarding ...  \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "df.to_csv(\"bitext_customer_support_dataset.csv\", index=False)\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0da7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | flags   | instruction                                                  | category   | intent       | response                                                                                                                                                                                                                                                                                                                           |\n",
      "|---:|:--------|:-------------------------------------------------------------|:-----------|:-------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 | B       | question about cancelling order {{Order Number}}             | ORDER      | cancel_order | I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.                                                                                                               |\n",
      "|  1 | BQZ     | i have a question about cancelling oorder {{Order Number}}   | ORDER      | cancel_order | I've been informed that you have a question about canceling order {{Order Number}}. I'm here to assist you! Please go ahead and let me know what specific question you have, and I'll provide you with all the information and guidance you need. Your satisfaction is my top priority.                                            |\n",
      "|  2 | BLQZ    | i need help cancelling puchase {{Order Number}}              | ORDER      | cancel_order | I can sense that you're seeking assistance with canceling your purchase with the purchase number {{Order Number}}. I apologize for any inconvenience caused, and I'm here to guide you through the process.                                                                                                                        |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | To cancel your purchase, please follow these steps:                                                                                                                                                                                                                                                                                |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | 1. Log into your account: Visit our {{Online Company Portal Info}} and sign in using your credentials.                                                                                                                                                                                                                             |\n",
      "|    |         |                                                              |            |              | 2. Locate your order: Once logged in, navigate to the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section to find the purchase with the order number {{Order Number}}.                                                                                                                                        |\n",
      "|    |         |                                                              |            |              | 3. Initiate the cancellation: Upon finding your purchase, look for the option labeled '{{Online Order Interaction}}' and click on it to initiate the cancellation process.                                                                                                                                                         |\n",
      "|    |         |                                                              |            |              | 4. Confirm the cancellation: Our system might ask for confirmation or feedback regarding the cancellation. Kindly provide the necessary information to complete the process.                                                                                                                                                       |\n",
      "|    |         |                                                              |            |              | 5. Seek additional support: If you encounter any difficulties or require further assistance, our dedicated customer support team is available during {{Customer Support Hours}}. You can contact us at {{Customer Support Phone Number}} or use the Live Chat feature on our {{Website URL}}.                                      |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | We appreciate your understanding and value your satisfaction. Should you have any more questions or concerns, feel free to reach out to us. We're here to help you.                                                                                                                                                                |\n",
      "|  3 | BL      | I need to cancel purchase {{Order Number}}                   | ORDER      | cancel_order | I understood that you need assistance with canceling your purchase with the order number {{Order Number}}. We apologize for any inconvenience this may have caused. To initiate the cancellation process, please follow these steps:                                                                                               |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | 1. Sign in to your account: Visit our {{Online Company Portal Info}} and login using your credentials.                                                                                                                                                                                                                             |\n",
      "|    |         |                                                              |            |              | 2. Access your order history: Once logged in, navigate to the \"{{Online Order Interaction}}\" or \"{{Online Order Interaction}}\" section.                                                                                                                                                                                            |\n",
      "|    |         |                                                              |            |              | 3. Locate the purchase: Look for the order with the specific order number {{Order Number}}.                                                                                                                                                                                                                                        |\n",
      "|    |         |                                                              |            |              | 4. Cancel the purchase: Click on the \"{{Online Order Interaction}}\" option associated with the purchase.                                                                                                                                                                                                                           |\n",
      "|    |         |                                                              |            |              | 5. Confirm the cancellation: Follow any further prompts or questions from the system to finalize the cancellation.                                                                                                                                                                                                                 |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | If you encounter any difficulties or have further questions, our dedicated team is available to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat feature on our website {{Website URL}}. We appreciate your understanding and value your satisfaction. |\n",
      "|  4 | BCELN   | I cannot afford this order, cancel purchase {{Order Number}} | ORDER      | cancel_order | I'm sensitive to the fact that you're facing financial difficulties and need to cancel your purchase with the order number {{Order Number}}. We're here to assist you with that, and I appreciate your transparency in sharing your situation with us. Let's proceed with the cancellation process:                                |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | 1. Access Your Account: Log in to our {{Online Company Portal Info}}.                                                                                                                                                                                                                                                              |\n",
      "|    |         |                                                              |            |              | 2. Locate Your Order History: Once you're logged in, navigate to the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section.                                                                                                                                                                                     |\n",
      "|    |         |                                                              |            |              | 3. Find the Relevant Purchase: Look for the purchase associated with the order number {{Order Number}} and click on it.                                                                                                                                                                                                            |\n",
      "|    |         |                                                              |            |              | 4. Initiate the Cancellation: Within the purchase details page, you should see an option labeled '{{Online Order Interaction}}'. Please select this option.                                                                                                                                                                        |\n",
      "|    |         |                                                              |            |              | 5. Confirm Cancellation: The system may prompt you for confirmation or ask a few questions for feedback. Please complete these steps.                                                                                                                                                                                              |\n",
      "|    |         |                                                              |            |              |                                                                                                                                                                                                                                                                                                                                    |\n",
      "|    |         |                                                              |            |              | If you encounter any difficulties or have further questions along the way, our team is ready to assist you. Reach out to us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or via Live Chat on our {{Website URL}}. We value your satisfaction and understand that situations can change.                  |\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"bitext_customer_support_dataset.csv\")\n",
    "print(df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff19075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine instruction and response to form training sequences\n",
    "df[\"text\"] = df[\"instruction\"].astype(str) + \" \" + df[\"response\"].astype(str)\n",
    "\n",
    "# Save to a plain text file, one sequence per line\n",
    "with open(\"train_sequences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df[\"text\"]:\n",
    "        f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aafc1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Setup the trainer with vocab size and special tokens\n",
    "trainer = trainers.BpeTrainer(vocab_size=1024, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
    "\n",
    "# Train from the file\n",
    "tokenizer.train(files=[\"train_sequences.txt\"], trainer=trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"bitext_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7981cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 → <pad>\n",
      "   1 → <s>\n",
      "   2 → </s>\n",
      "   3 → <unk>\n",
      "   4 → !\n",
      "   5 → \"\n",
      "   6 → #\n",
      "   7 → $\n",
      "   8 → &\n",
      "   9 → '\n",
      "  10 → (\n",
      "  11 → )\n",
      "  12 → *\n",
      "  13 → +\n",
      "  14 → ,\n",
      "  15 → -\n",
      "  16 → .\n",
      "  17 → /\n",
      "  18 → 0\n",
      "  19 → 1\n",
      "  20 → 2\n",
      "  21 → 3\n",
      "  22 → 4\n",
      "  23 → 5\n",
      "  24 → 6\n",
      "  25 → 7\n",
      "  26 → 8\n",
      "  27 → 9\n",
      "  28 → :\n",
      "  29 → ;\n",
      "  30 → >\n",
      "  31 → ?\n",
      "  32 → @\n",
      "  33 → A\n",
      "  34 → B\n",
      "  35 → C\n",
      "  36 → D\n",
      "  37 → E\n",
      "  38 → F\n",
      "  39 → G\n",
      "  40 → H\n",
      "  41 → I\n",
      "  42 → J\n",
      "  43 → K\n",
      "  44 → L\n",
      "  45 → M\n",
      "  46 → N\n",
      "  47 → O\n",
      "  48 → P\n",
      "  49 → Q\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer back\n",
    "tokenizer = Tokenizer.from_file(\"bitext_bpe_tokenizer.json\")\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Sort and show top 20 tokens\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "for token, idx in sorted_vocab[:50]:\n",
    "    print(f\"{idx:4} → {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a63afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer: Tokenizer) -> int:\n",
    "    \"\"\"\n",
    "    Returns the total vocabulary size including special tokens.\n",
    "    \"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    return len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41090399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['How', 'are', 'you', '?']\n",
      "IDs    : [934, 251, 116, 31]\n",
      "Decoded text: How are you ?\n"
     ]
    }
   ],
   "source": [
    "# Encode a sample text\n",
    "output = tokenizer.encode(\"How are you?\")\n",
    "\n",
    "# Print tokens and their IDs\n",
    "print(\"Tokens :\", output.tokens)\n",
    "print(\"IDs    :\", output.ids)\n",
    "\n",
    "print(\"Decoded text: \" + tokenizer.decode([934, 251, 116, 31]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84adcd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 91335\n",
      "Sample Token Lengths: [67, 81, 56, 0, 10, 0, 31, 38, 35, 32]\n",
      "Max Length: 362\n",
      "Min Length: 0\n",
      "Average Length: 49.40\n"
     ]
    }
   ],
   "source": [
    "# Read the training sequences\n",
    "with open(\"train_sequences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sequences = f.readlines()\n",
    "\n",
    "# Tokenize and print token lengths\n",
    "token_lengths = [len(tokenizer.encode(seq.strip()).tokens) for seq in sequences]\n",
    " \n",
    "print(f\"Total Sequences: {len(token_lengths)}\")\n",
    "print(\"Sample Token Lengths:\", token_lengths[:10])  # Print first 10 for a preview\n",
    " \n",
    "print(f\"Max Length: {max(token_lengths)}\")\n",
    "print(f\"Min Length: {min(token_lengths)}\")\n",
    "print(f\"Average Length: {sum(token_lengths) / len(token_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a35ec",
   "metadata": {},
   "source": [
    "### LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c68bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462035b9",
   "metadata": {},
   "source": [
    "1.Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "601eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        _, T, _ = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        weights = weights.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = weights @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b853b",
   "metadata": {},
   "source": [
    "2. Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ab8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701491b8",
   "metadata": {},
   "source": [
    "3. Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd57d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int) -> None:\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_forward = FeedFoward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff2d11",
   "metadata": {},
   "source": [
    "4. Assembling the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0a7134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_tokens: Tensor of token indices of shape (batch_size, sequence_length)\n",
    "            targets: Optional tensor of target token indices of same shape as input_tokens\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (logits, loss) where logits has shape (batch_size, sequence_length, vocab_size)\n",
    "            and loss is optional cross-entropy loss if targets are provided\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = input_tokens.shape\n",
    "\n",
    "        # input_tokens and targets are both (B,T) tensor of integers\n",
    "        token_embedding = self.token_embedding_table(input_tokens)  # (B,T,C)\n",
    "        positional_embedding = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = token_embedding + positional_embedding  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.final_layer_norm(x)  # (B,T,C)\n",
    "        logits = self.final_linear_layer(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "                Generate new tokens given a context.\n",
    "\n",
    "                Args:>ns: Starting token indices of shape (batch_size, sequence_length)\n",
    "                        max_new_tokens: Number of new tokens to generate\n",
    "\n",
    "                Returns:\n",
    "                        Tensor of token indices of shape (batch_size, sequence_length + max_new_tokens)\n",
    "                \"\"\"\n",
    "\n",
    "        # input_tokens is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop input_tokens to the last block_size tokens\n",
    "            cropped_input = input_tokens[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(cropped_input)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            input_tokens = torch.cat(\n",
    "                (input_tokens, idx_next), dim=1)  # (B, T+1)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380887fc",
   "metadata": {},
   "source": [
    "5. Parameters and Dummy Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49f04d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5264 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5e6ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 1024]) None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_length = 6\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "x = x.to(device)\n",
    "\n",
    "logits, loss = model(x)\n",
    "print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93cf2477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212dfb4b",
   "metadata": {},
   "source": [
    "Display the Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d483c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├─ token_embedding_table: Embedding (393,216 parameters)\n",
      "├─ position_embedding_table: Embedding (98,304 parameters)\n",
      "├─ blocks: Sequential (10,639,872 parameters)\n",
      "│  ├─ 0: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedFoward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 1: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedFoward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 2: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedFoward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 3: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedFoward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 4: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedFoward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 5: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedFoward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "├─ final_layer_norm: LayerNorm (768 parameters)\n",
      "├─ final_linear_layer: Linear (394,240 parameters)\n"
     ]
    }
   ],
   "source": [
    "def print_model_structure(model: torch.nn.Module, indent: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Custom function to print model structure in a hierarchical format\n",
    "    \"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        params = sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{indent}├─ {name}: {child.__class__.__name__} ({params:,} parameters)\")\n",
    "        print_model_structure(child, indent + '│  ')\n",
    "\n",
    "\n",
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f6d90",
   "metadata": {},
   "source": [
    "Generate and render computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc450e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved as model_graph.png\n"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.randint(0, vocab_size, (1, 128)).to(device)\n",
    "y, _ = model(x)  # Assuming your model returns (logits, loss)\n",
    "\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"model_graph\", format=\"png\")\n",
    "\n",
    "print(\"Graph saved as model_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bbb227c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Trainable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>token_embedding_table</td>\n",
       "      <td>Embedding</td>\n",
       "      <td>393216</td>\n",
       "      <td>393216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>position_embedding_table</td>\n",
       "      <td>Embedding</td>\n",
       "      <td>98304</td>\n",
       "      <td>98304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blocks.0.self_attention.heads.0.key</td>\n",
       "      <td>Linear</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blocks.0.self_attention.heads.0.query</td>\n",
       "      <td>Linear</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blocks.0.self_attention.heads.0.value</td>\n",
       "      <td>Linear</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>blocks.5.feed_forward.net.3</td>\n",
       "      <td>Dropout</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>blocks.5.layer_norm_1</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>blocks.5.layer_norm_2</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>final_layer_norm</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>final_linear_layer</td>\n",
       "      <td>Linear</td>\n",
       "      <td>394240</td>\n",
       "      <td>394240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Layer Name       Type  Parameters  Trainable\n",
       "0                    token_embedding_table  Embedding      393216     393216\n",
       "1                 position_embedding_table  Embedding       98304      98304\n",
       "2      blocks.0.self_attention.heads.0.key     Linear       24576      24576\n",
       "3    blocks.0.self_attention.heads.0.query     Linear       24576      24576\n",
       "4    blocks.0.self_attention.heads.0.value     Linear       24576      24576\n",
       "..                                     ...        ...         ...        ...\n",
       "191            blocks.5.feed_forward.net.3    Dropout           0          0\n",
       "192                  blocks.5.layer_norm_1  LayerNorm         768        768\n",
       "193                  blocks.5.layer_norm_2  LayerNorm         768        768\n",
       "194                       final_layer_norm  LayerNorm         768        768\n",
       "195                     final_linear_layer     Linear      394240     394240\n",
       "\n",
       "[196 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_stats(model: torch.nn.Module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame with detailed layer statistics\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Only leaf modules\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "            stats.append({\n",
    "                'Layer Name': name,\n",
    "                'Type': module.__class__.__name__,\n",
    "                'Parameters': params,\n",
    "                'Trainable': sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            })\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "stats_df = get_model_stats(model)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3988bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IITR_SCU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
