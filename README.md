
# YugAI â€“ GPT-based Customer Support SLM (Small Language Model)

YugAI is a mobile-friendly, GPT-inspired **Small Language Model (SLM)** designed from scratch for **customer support** use cases. Built with performance and efficiency in mind, YugAI uses **Byte Pair Encoding (BPE)** and is trained to stay within tight **memory constraints (>90%)**, making it ideal for edge/mobile deployment.

---

## ğŸš€ Features

- ğŸ§  Lightweight architecture with **13.78M parameters**
- ğŸ“± Mobile & edge-device friendly
- ğŸ”¤ Tokenized using **Byte Pair Encoding (BPE)**
- ğŸ› ï¸ Fine-tuned for customer support scenarios
- ğŸ’¾ Memory-efficient: uses >90% memory but avoids out-of-memory issues
- ğŸ“Š GFLOPs Theoretical Performance: **7.99 TFLOPs (~0.0079 PFLOPs)**

---

## ğŸ“ Model Architecture

- **Embedding Shapes**: `(128, 256)` Ã— 2
- **Total Parameters**: `13,785,088`
- **Tokens in Dataset**: `4,512,042`
- **Training Tokens (90%)**: `~4,060,837`
- **Batch Size**: `128 Ã— 256 = 32,768 tokens`
- **Training Steps**: `~124`

---

## âš™ï¸ Training Setup

- **Tokenizer**: Byte Pair Encoding (BPE)
- **Framework**: PyTorch
- **Compute**: CUDA (NVIDIA GPU)
- **Theoretical GFLOPs Calculation**:


GFLOPs = CUDA Cores Ã— Clock Speed Ã— 2
\= 3328 Ã— 1.2GHz Ã— 2
\= 7987.2 GFLOPs = \~7.99 TFLOPs


---

## ğŸ“¦ Use Cases

- Automated and intelligent **customer support chatbot**
- Context-aware **FAQ resolution**
- Mobile-based **text understanding** and response generation

---

## ğŸ§ª Future Work

- LoRA/Quantization for further optimization
- Deployment as a REST API or ONNX/TF Lite mobile model
- Multi-turn conversation handling

---

## ğŸ“„ License

This project is for research and educational purposes only.

---
