{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf12533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  1 14:46:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 571.59                 Driver Version: 571.59         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 12GB        WDDM  |   00000000:65:00.0  On |                  Off |\n",
      "| 30%   44C    P8              7W /   70W |    1073MiB /  12282MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2284    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A            3448    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A            4268    C+G   ...al\\Programs\\cursor\\Cursor.exe      N/A      |\n",
      "|    0   N/A  N/A            5356    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A            5744    C+G   ....0.3179.98\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A            7220    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A            8848    C+G   ...x64__8wekyb3d8bbwe\\Photos.exe      N/A      |\n",
      "|    0   N/A  N/A           11204    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A           11608    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12596    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           12612    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12940    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           13352    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           13728    C+G   ...crosoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           14756    C+G   ...64__8wekyb3d8bbwe\\Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A           15164    C+G   ...x64__8wekyb3d8bbwe\\Photos.exe      N/A      |\n",
      "|    0   N/A  N/A           15432    C+G   ...64__8wekyb3d8bbwe\\Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A           16984    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           17904    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           17996    C+G   ...4__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A           18580    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           18688    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18828    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A           20636    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A           21468    C+G   ...4__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A           22220    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           22848    C+G   ...64__8wekyb3d8bbwe\\Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A           23288    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           24304    C+G   ...ekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A           24976    C+G   ...h_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9709b5",
   "metadata": {},
   "source": [
    "95% > we will try to use the memory > 90% - so that not out of memory\n",
    "\n",
    "Theoretical GFLOPs = (CUDA Cores × Clock Speed in GHz × 2)\n",
    "\n",
    "GFLOPs = 3328 × 1.2 × 2 = **7987.2 GFLOPs** ≈ **7.99 TFLOPs** = **0.0079872 PFLOPs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf6e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"bitext_bpe_tokenizer.json\")\n",
    "\n",
    "def get_vocab_size(tokenizer: Tokenizer) -> int:\n",
    "    \"\"\"\n",
    "    Returns the total vocabulary size including special tokens.\n",
    "    \"\"\"\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    return len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecddaf",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a4a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's update the GPU configuration\n",
    "import torch\n",
    "torch.manual_seed(3647)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch._dynamo.config.suppress_errors = True \n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 for faster matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e075ea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.785088 M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformer.model import GPTLanguageModel\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 512\n",
    "n_head = 16\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    dropout=dropout,\n",
    "    device=device\n",
    ").to(device)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772efb97",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0724ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4512042"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"train_sequences.txt\", \"r\") as f:\n",
    "    text_sequence = f.read()\n",
    "\n",
    "encoded_text_sequence = tokenizer.encode(text_sequence)\n",
    "len(encoded_text_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593dc93e",
   "metadata": {},
   "source": [
    "train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cae1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoded_text_sequence.ids, dtype=torch.long)\n",
    "\n",
    "# Split into train/validation\n",
    "split_index = int(0.9 * len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556e24f",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db07d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, block_size: int) -> None:\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.data[index:index + self.block_size]\n",
    "        y = self.data[index + 1:index + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(\n",
    "        train_data: torch.Tensor,\n",
    "        val_data: torch.Tensor,\n",
    "        block_size: int,\n",
    "        batch_size: int,\n",
    "        device: torch.device\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_dataset = TextDataset(train_data.to(device), block_size)\n",
    "    val_dataset = TextDataset(val_data.to(device), block_size)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206a37f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 256]), torch.Size([128, 256]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa396f",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7a9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    eval_iters: int\n",
    ") -> Dict[str, float]:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= eval_iters:\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                _, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        output[split] = losses.mean().item()\n",
    "\n",
    "    model.train()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcda8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    model: GPTLanguageModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    loss: float,\n",
    "    file_path: str = \"checkpoint.pth\"\n",
    ") -> None:\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6773af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / step 0: train loss 7.0336, val loss 7.0990\n",
      "iteration 0 / step 100: train loss 2.8574, val loss 3.2870\n",
      "iteration 0 / step 200: train loss 2.1408, val loss 2.4412\n",
      "iteration 0 / step 300: train loss 1.5602, val loss 1.9031\n",
      "iteration 0 / step 400: train loss 1.2852, val loss 1.6078\n",
      "iteration 0 / step 500: train loss 1.1404, val loss 1.4414\n",
      "iteration 0 / step 600: train loss 1.0565, val loss 1.3289\n",
      "iteration 0 / step 700: train loss 0.9899, val loss 1.2505\n",
      "iteration 0 / step 800: train loss 0.9420, val loss 1.2023\n",
      "iteration 0 / step 900: train loss 0.9053, val loss 1.1487\n",
      "iteration 0 / step 1000: train loss 0.8667, val loss 1.1181\n",
      "iteration 0 / step 1100: train loss 0.8374, val loss 1.0889\n",
      "iteration 0 / step 1200: train loss 0.8166, val loss 1.0714\n",
      "iteration 0 / step 1300: train loss 0.7985, val loss 1.0537\n",
      "iteration 0 / step 1400: train loss 0.7812, val loss 1.0424\n",
      "iteration 0 / step 1500: train loss 0.7653, val loss 1.0244\n",
      "iteration 0 / step 1600: train loss 0.7484, val loss 1.0098\n",
      "iteration 0 / step 1700: train loss 0.7373, val loss 1.0023\n",
      "iteration 0 / step 1800: train loss 0.7243, val loss 0.9939\n",
      "iteration 0 / step 1900: train loss 0.7179, val loss 0.9756\n",
      "iteration 0 / step 2000: train loss 0.7036, val loss 0.9725\n",
      "iteration 0 / step 2100: train loss 0.6931, val loss 0.9680\n",
      "iteration 0 / step 2200: train loss 0.6859, val loss 0.9567\n",
      "iteration 0 / step 2300: train loss 0.6803, val loss 0.9551\n",
      "iteration 0 / step 2400: train loss 0.6701, val loss 0.9435\n",
      "iteration 0 / step 2500: train loss 0.6614, val loss 0.9378\n",
      "iteration 0 / step 2600: train loss 0.6543, val loss 0.9431\n",
      "iteration 0 / step 2700: train loss 0.6474, val loss 0.9339\n",
      "iteration 0 / step 2800: train loss 0.6408, val loss 0.9323\n",
      "iteration 0 / step 2900: train loss 0.6324, val loss 0.9241\n",
      "iteration 0 / step 3000: train loss 0.6292, val loss 0.9273\n",
      "iteration 0 / step 3100: train loss 0.6233, val loss 0.9190\n",
      "iteration 0 / step 3200: train loss 0.6173, val loss 0.9184\n",
      "iteration 0 / step 3300: train loss 0.6128, val loss 0.9050\n",
      "iteration 0 / step 3400: train loss 0.6077, val loss 0.9117\n",
      "iteration 0 / step 3500: train loss 0.6041, val loss 0.9036\n",
      "iteration 0 / step 3600: train loss 0.5973, val loss 0.9035\n",
      "iteration 0 / step 3700: train loss 0.5956, val loss 0.9059\n",
      "iteration 0 / step 3800: train loss 0.5886, val loss 0.8953\n",
      "iteration 0 / step 3900: train loss 0.5831, val loss 0.9033\n",
      "iteration 0 / step 4000: train loss 0.5813, val loss 0.9038\n",
      "iteration 0 / step 4100: train loss 0.5774, val loss 0.9096\n",
      "iteration 0 / step 4200: train loss 0.5718, val loss 0.9028\n",
      "iteration 0 / step 4300: train loss 0.5694, val loss 0.8909\n",
      "iteration 0 / step 4400: train loss 0.5664, val loss 0.8865\n",
      "iteration 0 / step 4500: train loss 0.5623, val loss 0.8895\n",
      "iteration 0 / step 4600: train loss 0.5610, val loss 0.8841\n",
      "iteration 0 / step 4700: train loss 0.5546, val loss 0.8799\n",
      "iteration 0 / step 4800: train loss 0.5504, val loss 0.8814\n",
      "iteration 0 / step 4900: train loss 0.5486, val loss 0.8849\n",
      "iteration 0 / step 5000: train loss 0.5453, val loss 0.8803\n",
      "iteration 0 / step 5100: train loss 0.5438, val loss 0.8893\n",
      "iteration 0 / step 5200: train loss 0.5415, val loss 0.8848\n",
      "iteration 0 / step 5300: train loss 0.5358, val loss 0.8810\n",
      "iteration 0 / step 5400: train loss 0.5326, val loss 0.8901\n",
      "iteration 0 / step 5500: train loss 0.5316, val loss 0.8828\n",
      "iteration 0 / step 5600: train loss 0.5283, val loss 0.8897\n",
      "iteration 0 / step 5700: train loss 0.5262, val loss 0.8835\n",
      "iteration 0 / step 5800: train loss 0.5235, val loss 0.8881\n",
      "iteration 0 / step 5900: train loss 0.5214, val loss 0.8817\n",
      "iteration 0 / step 6000: train loss 0.5179, val loss 0.8858\n",
      "iteration 0 / step 6100: train loss 0.5141, val loss 0.8834\n",
      "iteration 0 / step 6200: train loss 0.5144, val loss 0.8830\n",
      "iteration 0 / step 6300: train loss 0.5116, val loss 0.8942\n",
      "iteration 0 / step 6400: train loss 0.5075, val loss 0.8850\n",
      "iteration 0 / step 6500: train loss 0.5063, val loss 0.8850\n",
      "iteration 0 / step 6600: train loss 0.5045, val loss 0.8880\n",
      "iteration 0 / step 6700: train loss 0.5018, val loss 0.8842\n",
      "iteration 0 / step 6800: train loss 0.5002, val loss 0.8807\n",
      "iteration 0 / step 6900: train loss 0.4968, val loss 0.8830\n",
      "iteration 0 / step 7000: train loss 0.4955, val loss 0.8838\n",
      "iteration 0 / step 7100: train loss 0.4920, val loss 0.8814\n",
      "iteration 0 / step 7200: train loss 0.4908, val loss 0.8788\n",
      "iteration 0 / step 7300: train loss 0.4894, val loss 0.8799\n",
      "iteration 0 / step 7400: train loss 0.4863, val loss 0.8788\n",
      "iteration 0 / step 7500: train loss 0.4853, val loss 0.8827\n",
      "iteration 0 / step 7600: train loss 0.4812, val loss 0.8828\n",
      "iteration 0 / step 7700: train loss 0.4798, val loss 0.8852\n",
      "iteration 0 / step 7800: train loss 0.4798, val loss 0.8841\n",
      "iteration 0 / step 7900: train loss 0.4763, val loss 0.8834\n",
      "iteration 0 / step 8000: train loss 0.4747, val loss 0.8859\n",
      "iteration 0 / step 8100: train loss 0.4726, val loss 0.8870\n",
      "iteration 0 / step 8200: train loss 0.4699, val loss 0.8876\n",
      "iteration 0 / step 8300: train loss 0.4676, val loss 0.8858\n",
      "iteration 0 / step 8400: train loss 0.4676, val loss 0.8847\n",
      "iteration 0 / step 8500: train loss 0.4660, val loss 0.8843\n",
      "iteration 0 / step 8600: train loss 0.4636, val loss 0.8846\n",
      "iteration 0 / step 8700: train loss 0.4610, val loss 0.8806\n",
      "iteration 0 / step 8800: train loss 0.4609, val loss 0.8873\n",
      "iteration 0 / step 8900: train loss 0.4591, val loss 0.8823\n",
      "iteration 0 / step 9000: train loss 0.4571, val loss 0.8865\n",
      "iteration 0 / step 9100: train loss 0.4559, val loss 0.8832\n",
      "iteration 0 / step 9200: train loss 0.4537, val loss 0.8869\n",
      "iteration 0 / step 9300: train loss 0.4528, val loss 0.8795\n",
      "iteration 0 / step 9400: train loss 0.4508, val loss 0.8892\n",
      "iteration 0 / step 9500: train loss 0.4510, val loss 0.8867\n",
      "iteration 0 / step 9600: train loss 0.4468, val loss 0.8909\n",
      "iteration 0 / step 9700: train loss 0.4466, val loss 0.8928\n",
      "iteration 0 / step 9800: train loss 0.4463, val loss 0.8948\n",
      "iteration 0 / step 9900: train loss 0.4428, val loss 0.8896\n",
      "iteration 0 / step 10000: train loss 0.4432, val loss 0.8911\n",
      "iteration 0 / step 10100: train loss 0.4405, val loss 0.8878\n",
      "iteration 0 / step 10200: train loss 0.4386, val loss 0.8923\n",
      "iteration 0 / step 10300: train loss 0.4372, val loss 0.8943\n",
      "iteration 0 / step 10400: train loss 0.4359, val loss 0.8920\n",
      "iteration 0 / step 10500: train loss 0.4330, val loss 0.8870\n",
      "iteration 0 / step 10600: train loss 0.4321, val loss 0.8832\n",
      "iteration 0 / step 10700: train loss 0.4322, val loss 0.9007\n",
      "iteration 0 / step 10800: train loss 0.4317, val loss 0.8944\n",
      "iteration 0 / step 10900: train loss 0.4282, val loss 0.8917\n",
      "iteration 0 / step 11000: train loss 0.4271, val loss 0.8933\n",
      "iteration 0 / step 11100: train loss 0.4273, val loss 0.8963\n",
      "iteration 0 / step 11200: train loss 0.4234, val loss 0.8960\n",
      "iteration 0 / step 11300: train loss 0.4228, val loss 0.8947\n",
      "iteration 0 / step 11400: train loss 0.4221, val loss 0.8892\n",
      "iteration 0 / step 11500: train loss 0.4218, val loss 0.8931\n",
      "iteration 0 / step 11600: train loss 0.4216, val loss 0.8945\n",
      "iteration 0 / step 11700: train loss 0.4172, val loss 0.8920\n",
      "iteration 0 / step 11800: train loss 0.4160, val loss 0.8905\n",
      "iteration 0 / step 11900: train loss 0.4164, val loss 0.8938\n",
      "iteration 0 / step 12000: train loss 0.4147, val loss 0.8953\n",
      "iteration 0 / step 12100: train loss 0.4136, val loss 0.9018\n",
      "iteration 0 / step 12200: train loss 0.4123, val loss 0.9014\n",
      "iteration 0 / step 12300: train loss 0.4109, val loss 0.8948\n",
      "iteration 0 / step 12400: train loss 0.4095, val loss 0.9016\n",
      "iteration 0 / step 12500: train loss 0.4076, val loss 0.8960\n",
      "iteration 0 / step 12600: train loss 0.4068, val loss 0.9005\n",
      "iteration 0 / step 12700: train loss 0.4051, val loss 0.9024\n",
      "iteration 0 / step 12800: train loss 0.4039, val loss 0.9062\n",
      "iteration 0 / step 12900: train loss 0.4022, val loss 0.9013\n",
      "iteration 0 / step 13000: train loss 0.4031, val loss 0.9002\n",
      "iteration 0 / step 13100: train loss 0.4009, val loss 0.9017\n",
      "iteration 0 / step 13200: train loss 0.3997, val loss 0.8978\n",
      "iteration 0 / step 13300: train loss 0.3983, val loss 0.8998\n",
      "iteration 0 / step 13400: train loss 0.3959, val loss 0.9073\n",
      "iteration 0 / step 13500: train loss 0.3970, val loss 0.9054\n",
      "iteration 0 / step 13600: train loss 0.3957, val loss 0.9061\n",
      "iteration 0 / step 13700: train loss 0.3945, val loss 0.9105\n",
      "iteration 0 / step 13800: train loss 0.3923, val loss 0.9029\n",
      "iteration 0 / step 13900: train loss 0.3905, val loss 0.9080\n",
      "iteration 0 / step 14000: train loss 0.3932, val loss 0.9105\n",
      "iteration 0 / step 14100: train loss 0.3901, val loss 0.9092\n",
      "iteration 0 / step 14200: train loss 0.3889, val loss 0.9110\n",
      "iteration 0 / step 14300: train loss 0.3869, val loss 0.9103\n",
      "iteration 0 / step 14400: train loss 0.3869, val loss 0.9174\n",
      "iteration 0 / step 14500: train loss 0.3871, val loss 0.9136\n",
      "iteration 0 / step 14600: train loss 0.3855, val loss 0.9094\n",
      "iteration 0 / step 14700: train loss 0.3840, val loss 0.9072\n",
      "iteration 0 / step 14800: train loss 0.3831, val loss 0.9081\n",
      "iteration 0 / step 14900: train loss 0.3826, val loss 0.9161\n",
      "iteration 0 / step 15000: train loss 0.3813, val loss 0.9119\n",
      "iteration 0 / step 15100: train loss 0.3772, val loss 0.9085\n",
      "iteration 0 / step 15200: train loss 0.3798, val loss 0.9143\n",
      "iteration 0 / step 15300: train loss 0.3763, val loss 0.9195\n",
      "iteration 0 / step 15400: train loss 0.3757, val loss 0.9215\n",
      "iteration 0 / step 15500: train loss 0.3750, val loss 0.9089\n",
      "iteration 0 / step 15600: train loss 0.3741, val loss 0.9197\n",
      "iteration 0 / step 15700: train loss 0.3726, val loss 0.9161\n",
      "iteration 0 / step 15800: train loss 0.3727, val loss 0.9202\n",
      "iteration 0 / step 15900: train loss 0.3727, val loss 0.9154\n",
      "iteration 0 / step 16000: train loss 0.3695, val loss 0.9191\n",
      "iteration 0 / step 16100: train loss 0.3700, val loss 0.9233\n",
      "iteration 0 / step 16200: train loss 0.3699, val loss 0.9224\n",
      "iteration 0 / step 16300: train loss 0.3685, val loss 0.9166\n",
      "iteration 0 / step 16400: train loss 0.3670, val loss 0.9260\n",
      "iteration 0 / step 16500: train loss 0.3660, val loss 0.9200\n",
      "iteration 0 / step 16600: train loss 0.3646, val loss 0.9239\n",
      "iteration 0 / step 16700: train loss 0.3645, val loss 0.9219\n",
      "iteration 0 / step 16800: train loss 0.3642, val loss 0.9188\n",
      "iteration 0 / step 16900: train loss 0.3631, val loss 0.9239\n",
      "iteration 0 / step 17000: train loss 0.3632, val loss 0.9164\n",
      "iteration 0 / step 17100: train loss 0.3627, val loss 0.9214\n",
      "iteration 0 / step 17200: train loss 0.3621, val loss 0.9235\n",
      "iteration 0 / step 17300: train loss 0.3596, val loss 0.9277\n",
      "iteration 0 / step 17400: train loss 0.3590, val loss 0.9276\n",
      "iteration 0 / step 17500: train loss 0.3580, val loss 0.9284\n",
      "iteration 0 / step 17600: train loss 0.3570, val loss 0.9290\n",
      "iteration 0 / step 17700: train loss 0.3565, val loss 0.9263\n",
      "iteration 0 / step 17800: train loss 0.3546, val loss 0.9274\n",
      "iteration 0 / step 17900: train loss 0.3543, val loss 0.9253\n",
      "iteration 0 / step 18000: train loss 0.3541, val loss 0.9303\n",
      "iteration 0 / step 18100: train loss 0.3534, val loss 0.9224\n",
      "iteration 0 / step 18200: train loss 0.3532, val loss 0.9292\n",
      "iteration 0 / step 18300: train loss 0.3529, val loss 0.9239\n",
      "iteration 0 / step 18400: train loss 0.3508, val loss 0.9277\n",
      "iteration 0 / step 18500: train loss 0.3497, val loss 0.9311\n",
      "iteration 0 / step 18600: train loss 0.3480, val loss 0.9320\n",
      "iteration 0 / step 18700: train loss 0.3497, val loss 0.9372\n",
      "iteration 0 / step 18800: train loss 0.3470, val loss 0.9313\n",
      "iteration 0 / step 18900: train loss 0.3482, val loss 0.9308\n",
      "iteration 0 / step 19000: train loss 0.3454, val loss 0.9314\n",
      "iteration 0 / step 19100: train loss 0.3459, val loss 0.9387\n",
      "iteration 0 / step 19200: train loss 0.3457, val loss 0.9260\n",
      "iteration 0 / step 19300: train loss 0.3447, val loss 0.9395\n",
      "iteration 0 / step 19400: train loss 0.3408, val loss 0.9384\n",
      "iteration 0 / step 19500: train loss 0.3420, val loss 0.9349\n",
      "iteration 0 / step 19600: train loss 0.3417, val loss 0.9411\n",
      "iteration 0 / step 19700: train loss 0.3396, val loss 0.9339\n",
      "iteration 0 / step 19800: train loss 0.3395, val loss 0.9428\n",
      "iteration 0 / step 19900: train loss 0.3408, val loss 0.9424\n",
      "iteration 0 / step 20000: train loss 0.3392, val loss 0.9432\n",
      "iteration 0 / step 20100: train loss 0.3375, val loss 0.9417\n",
      "iteration 0 / step 20200: train loss 0.3365, val loss 0.9374\n",
      "iteration 0 / step 20300: train loss 0.3360, val loss 0.9400\n",
      "iteration 0 / step 20400: train loss 0.3347, val loss 0.9434\n",
      "iteration 0 / step 20500: train loss 0.3346, val loss 0.9400\n",
      "iteration 0 / step 20600: train loss 0.3332, val loss 0.9490\n",
      "iteration 0 / step 20700: train loss 0.3333, val loss 0.9448\n",
      "iteration 0 / step 20800: train loss 0.3325, val loss 0.9433\n",
      "iteration 0 / step 20900: train loss 0.3319, val loss 0.9457\n",
      "iteration 0 / step 21000: train loss 0.3311, val loss 0.9524\n",
      "iteration 0 / step 21100: train loss 0.3310, val loss 0.9515\n",
      "iteration 0 / step 21200: train loss 0.3306, val loss 0.9519\n",
      "iteration 0 / step 21300: train loss 0.3296, val loss 0.9464\n",
      "iteration 0 / step 21400: train loss 0.3280, val loss 0.9481\n",
      "iteration 0 / step 21500: train loss 0.3274, val loss 0.9522\n",
      "iteration 0 / step 21600: train loss 0.3285, val loss 0.9447\n",
      "iteration 0 / step 21700: train loss 0.3249, val loss 0.9526\n",
      "iteration 0 / step 21800: train loss 0.3259, val loss 0.9502\n",
      "iteration 0 / step 21900: train loss 0.3252, val loss 0.9438\n",
      "iteration 0 / step 22000: train loss 0.3244, val loss 0.9484\n",
      "iteration 0 / step 22100: train loss 0.3233, val loss 0.9527\n",
      "iteration 0 / step 22200: train loss 0.3226, val loss 0.9486\n",
      "iteration 0 / step 22300: train loss 0.3222, val loss 0.9467\n",
      "iteration 0 / step 22400: train loss 0.3224, val loss 0.9504\n",
      "iteration 0 / step 22500: train loss 0.3206, val loss 0.9505\n",
      "iteration 0 / step 22600: train loss 0.3202, val loss 0.9526\n",
      "iteration 0 / step 22700: train loss 0.3192, val loss 0.9591\n",
      "iteration 0 / step 22800: train loss 0.3183, val loss 0.9503\n",
      "iteration 0 / step 22900: train loss 0.3184, val loss 0.9567\n",
      "iteration 0 / step 23000: train loss 0.3183, val loss 0.9571\n",
      "iteration 0 / step 23100: train loss 0.3163, val loss 0.9615\n",
      "iteration 0 / step 23200: train loss 0.3171, val loss 0.9623\n",
      "iteration 0 / step 23300: train loss 0.3155, val loss 0.9524\n",
      "iteration 0 / step 23400: train loss 0.3151, val loss 0.9623\n",
      "iteration 0 / step 23500: train loss 0.3144, val loss 0.9646\n",
      "iteration 0 / step 23600: train loss 0.3140, val loss 0.9649\n",
      "iteration 0 / step 23700: train loss 0.3134, val loss 0.9656\n",
      "iteration 0 / step 23800: train loss 0.3131, val loss 0.9616\n",
      "iteration 0 / step 23900: train loss 0.3124, val loss 0.9684\n",
      "iteration 0 / step 24000: train loss 0.3113, val loss 0.9674\n",
      "iteration 0 / step 24100: train loss 0.3109, val loss 0.9675\n",
      "iteration 0 / step 24200: train loss 0.3103, val loss 0.9594\n",
      "iteration 0 / step 24300: train loss 0.3096, val loss 0.9681\n",
      "iteration 0 / step 24400: train loss 0.3077, val loss 0.9654\n",
      "iteration 0 / step 24500: train loss 0.3104, val loss 0.9669\n",
      "iteration 0 / step 24600: train loss 0.3077, val loss 0.9633\n",
      "iteration 0 / step 24700: train loss 0.3071, val loss 0.9622\n",
      "iteration 0 / step 24800: train loss 0.3072, val loss 0.9693\n",
      "iteration 0 / step 24900: train loss 0.3069, val loss 0.9721\n",
      "iteration 0 / step 25000: train loss 0.3068, val loss 0.9689\n",
      "iteration 0 / step 25100: train loss 0.3056, val loss 0.9696\n",
      "iteration 0 / step 25200: train loss 0.3055, val loss 0.9653\n",
      "iteration 0 / step 25300: train loss 0.3045, val loss 0.9664\n",
      "iteration 0 / step 25400: train loss 0.3033, val loss 0.9694\n",
      "iteration 0 / step 25500: train loss 0.3029, val loss 0.9719\n",
      "iteration 0 / step 25600: train loss 0.3033, val loss 0.9717\n",
      "iteration 0 / step 25700: train loss 0.3015, val loss 0.9764\n",
      "iteration 0 / step 25800: train loss 0.3018, val loss 0.9677\n",
      "iteration 0 / step 25900: train loss 0.3012, val loss 0.9800\n",
      "iteration 0 / step 26000: train loss 0.2994, val loss 0.9727\n",
      "iteration 0 / step 26100: train loss 0.2986, val loss 0.9838\n",
      "iteration 0 / step 26200: train loss 0.2997, val loss 0.9760\n",
      "iteration 0 / step 26300: train loss 0.2981, val loss 0.9837\n",
      "iteration 0 / step 26400: train loss 0.2985, val loss 0.9743\n",
      "iteration 0 / step 26500: train loss 0.2973, val loss 0.9801\n",
      "iteration 0 / step 26600: train loss 0.2981, val loss 0.9795\n",
      "iteration 0 / step 26700: train loss 0.2967, val loss 0.9775\n",
      "iteration 0 / step 26800: train loss 0.2944, val loss 0.9798\n",
      "iteration 0 / step 26900: train loss 0.2947, val loss 0.9811\n",
      "iteration 0 / step 27000: train loss 0.2947, val loss 0.9785\n",
      "iteration 0 / step 27100: train loss 0.2946, val loss 0.9832\n",
      "iteration 0 / step 27200: train loss 0.2939, val loss 0.9755\n",
      "iteration 0 / step 27300: train loss 0.2947, val loss 0.9798\n",
      "iteration 0 / step 27400: train loss 0.2925, val loss 0.9785\n",
      "iteration 0 / step 27500: train loss 0.2925, val loss 0.9795\n",
      "iteration 0 / step 27600: train loss 0.2921, val loss 0.9800\n",
      "iteration 0 / step 27700: train loss 0.2916, val loss 0.9818\n",
      "iteration 0 / step 27800: train loss 0.2915, val loss 0.9780\n",
      "iteration 0 / step 27900: train loss 0.2896, val loss 0.9882\n",
      "iteration 0 / step 28000: train loss 0.2910, val loss 0.9812\n",
      "iteration 0 / step 28100: train loss 0.2898, val loss 0.9766\n",
      "iteration 0 / step 28200: train loss 0.2885, val loss 0.9851\n",
      "iteration 0 / step 28300: train loss 0.2888, val loss 0.9850\n",
      "iteration 0 / step 28400: train loss 0.2888, val loss 0.9838\n",
      "iteration 0 / step 28500: train loss 0.2877, val loss 0.9888\n",
      "iteration 0 / step 28600: train loss 0.2862, val loss 0.9868\n",
      "iteration 0 / step 28700: train loss 0.2858, val loss 0.9915\n",
      "iteration 0 / step 28800: train loss 0.2852, val loss 0.9900\n",
      "iteration 0 / step 28900: train loss 0.2840, val loss 0.9889\n",
      "iteration 0 / step 29000: train loss 0.2855, val loss 0.9918\n",
      "iteration 0 / step 29100: train loss 0.2843, val loss 0.9891\n",
      "iteration 0 / step 29200: train loss 0.2833, val loss 0.9908\n",
      "iteration 0 / step 29300: train loss 0.2855, val loss 0.9820\n",
      "iteration 0 / step 29400: train loss 0.2831, val loss 0.9947\n",
      "iteration 0 / step 29500: train loss 0.2833, val loss 0.9923\n",
      "iteration 0 / step 29600: train loss 0.2813, val loss 0.9945\n",
      "iteration 0 / step 29700: train loss 0.2816, val loss 0.9930\n",
      "iteration 0 / step 29800: train loss 0.2826, val loss 0.9982\n",
      "iteration 0 / step 29900: train loss 0.2822, val loss 0.9934\n",
      "iteration 0 / step 30000: train loss 0.2800, val loss 0.9926\n",
      "iteration 0 / step 30100: train loss 0.2808, val loss 0.9944\n",
      "iteration 0 / step 30200: train loss 0.2801, val loss 0.9931\n",
      "iteration 0 / step 30300: train loss 0.2791, val loss 1.0014\n",
      "iteration 0 / step 30400: train loss 0.2792, val loss 0.9998\n",
      "iteration 0 / step 30500: train loss 0.2772, val loss 1.0032\n",
      "iteration 0 / step 30600: train loss 0.2785, val loss 1.0055\n",
      "iteration 0 / step 30700: train loss 0.2766, val loss 1.0019\n",
      "iteration 0 / step 30800: train loss 0.2760, val loss 1.0080\n",
      "iteration 0 / step 30900: train loss 0.2753, val loss 0.9989\n",
      "iteration 0 / step 31000: train loss 0.2763, val loss 0.9944\n",
      "iteration 0 / step 31100: train loss 0.2758, val loss 1.0049\n",
      "iteration 0 / step 31200: train loss 0.2752, val loss 1.0093\n",
      "iteration 0 / step 31300: train loss 0.2745, val loss 1.0053\n",
      "iteration 0 / step 31400: train loss 0.2744, val loss 1.0097\n",
      "iteration 0 / step 31500: train loss 0.2743, val loss 0.9939\n",
      "iteration 0 / step 31600: train loss 0.2730, val loss 1.0086\n",
      "iteration 0 / step 31700: train loss 0.2746, val loss 1.0048\n",
      "iteration 0 / step 31723: train loss 0.2743, val loss 1.0027\n",
      "Epoch 1 average training loss: 0.5717\n",
      "Epoch 1 training time: 57428.18 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../output/pre_training/run_4 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m average training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavg_epoch_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../output/pre_training/run_4/checkpoint_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miteration\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[1;34m(model, optimizer, epoch, loss, file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_checkpoint\u001b[39m(\n\u001b[0;32m      2\u001b[0m     model: GPTLanguageModel,\n\u001b[0;32m      3\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     file_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\n\u001b[0;32m     13\u001b[0m     }\n\u001b[1;32m---> 14\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\.conda\\envs\\IITR_SCU\\lib\\site-packages\\torch\\serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\.conda\\envs\\IITR_SCU\\lib\\site-packages\\torch\\serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\.conda\\envs\\IITR_SCU\\lib\\site-packages\\torch\\serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ../output/pre_training/run_4 does not exist."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "max_iters = 1\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "learning_rate = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_loader, val_loader = get_dataloaders(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        if batch_idx % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
    "            losses = estimate_loss(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                eval_iters=min(eval_iters, len(val_loader))\n",
    "            )\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(\n",
    "                f\"iteration {iteration} / step {batch_idx}: \"\n",
    "                f\"train loss {losses['train']:.4f}, \"\n",
    "                f\"val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "\n",
    "        logits, loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {iteration + 1} average training loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"Epoch {iteration + 1} training time: {elapsed:.2f} seconds\")\n",
    "\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=iteration,\n",
    "        loss=avg_epoch_loss,\n",
    "        file_path=f\"../output/pre_training/run_4/checkpoint_{iteration}.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef13fa6",
   "metadata": {},
   "source": [
    "- len(encoded_text_sequence) = 4,512,042 tokens\n",
    "- So training data (90%) = 0.9 * 4,512,042 ≈ 4,060,837 tokens\n",
    "- Tokens per batch = 128 * 256 = 32,768\n",
    "- total_steps = 4,060,837 / 32,768 ≈ 124 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93fc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=iteration,\n",
    "        loss=avg_epoch_loss,\n",
    "        file_path=\"E:/Vivek/YugAI/output/checkpoint_1.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b942ad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(val_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0d8de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H i reg direct don De ve gor ong associated App ️ jo reach down Click by cancel err de ok reach receive polog relevant sti bsite ple Ch ist reat indly feel there feel ized follow compens son tain gether imbur : contact ively go product Q ha z error of T Su where In assu ( get 💡 If ts enef Could ve i imb pen operation ty res 1 us lease indly sit ; 🌟 Name es ad payment confirm person ctions ef Q av elig indly changes hesitate 🙏 term bus some deli about detailed cur settings\n"
     ]
    }
   ],
   "source": [
    "# Encode text and extract token IDs\n",
    "\n",
    "flag = \"1\"\n",
    "\n",
    "while(flag == \"1\"):\n",
    "    query = input(\"Enter you query\")\n",
    "    input_tokens = tokenizer.encode(query).ids\n",
    "\n",
    "    # Convert to tensor\n",
    "    input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)  # shape: (1, T)\n",
    "\n",
    "    # Generate text\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_tokens=input_tokens, max_new_tokens=100)\n",
    "\n",
    "    # Decode and print\n",
    "    print(tokenizer.decode(output[0].tolist()))\n",
    "    flag = input(\"Do you have another query or not 0/1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972c32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IITR_SCU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
